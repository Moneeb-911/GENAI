{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10173648,"sourceType":"datasetVersion","datasetId":6283626}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers accelerate torch scikit-learn matplotlib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:23:40.946831Z","iopub.execute_input":"2024-12-11T18:23:40.947393Z","iopub.status.idle":"2024-12-11T18:23:50.792479Z","shell.execute_reply.started":"2024-12-11T18:23:40.947364Z","shell.execute_reply":"2024-12-11T18:23:50.791403Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:23:50.794775Z","iopub.execute_input":"2024-12-11T18:23:50.795251Z","iopub.status.idle":"2024-12-11T18:23:54.375187Z","shell.execute_reply.started":"2024-12-11T18:23:50.795210Z","shell.execute_reply":"2024-12-11T18:23:54.374359Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('cats_vs_dogs')\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:23:54.376403Z","iopub.execute_input":"2024-12-11T18:23:54.376876Z","iopub.status.idle":"2024-12-11T18:24:03.176185Z","shell.execute_reply.started":"2024-12-11T18:23:54.376838Z","shell.execute_reply":"2024-12-11T18:24:03.175338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e10fadc0a3464bb66dcb4360712b32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71fa2ab2d1049cb83de9abc5e2f57f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/391M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fddc9cb152640eab31ea4c55b232aa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/23410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd7d8bffccc42c2b1022310847eab53"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'labels'],\n        num_rows: 23410\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset['train'].features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:24:03.177725Z","iopub.execute_input":"2024-12-11T18:24:03.178169Z","iopub.status.idle":"2024-12-11T18:24:03.183927Z","shell.execute_reply.started":"2024-12-11T18:24:03.178141Z","shell.execute_reply":"2024-12-11T18:24:03.183056Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'image': Image(mode=None, decode=True, id=None),\n 'labels': ClassLabel(names=['cat', 'dog'], id=None)}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Split the training data into train and test (let's say 10% for the test set)\ntrain_test_split = dataset['train'].train_test_split(test_size=0.1)\n\n# Further split the training set to get a validation set (e.g., 10% of the training set)\ntrain_val_split = train_test_split['train'].train_test_split(test_size=0.1)\n\n# Combine the splits into a new DatasetDict\nfinal_dataset = {\n    'train': train_val_split['train'],\n    'val': train_val_split['test'],  \n    'test': train_test_split['test']  \n}\n\nfinal_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:24:03.184833Z","iopub.execute_input":"2024-12-11T18:24:03.185106Z","iopub.status.idle":"2024-12-11T18:24:03.538696Z","shell.execute_reply.started":"2024-12-11T18:24:03.185081Z","shell.execute_reply":"2024-12-11T18:24:03.537870Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'train': Dataset({\n     features: ['image', 'labels'],\n     num_rows: 18962\n }),\n 'val': Dataset({\n     features: ['image', 'labels'],\n     num_rows: 2107\n }),\n 'test': Dataset({\n     features: ['image', 'labels'],\n     num_rows: 2341\n })}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_ds = final_dataset[\"train\"]\nval_ds = final_dataset[\"val\"]\ntest_ds = final_dataset[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:24:49.456771Z","iopub.execute_input":"2024-12-11T18:24:49.457578Z","iopub.status.idle":"2024-12-11T18:24:49.461464Z","shell.execute_reply.started":"2024-12-11T18:24:49.457542Z","shell.execute_reply":"2024-12-11T18:24:49.460636Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:25:43.566911Z","iopub.execute_input":"2024-12-11T18:25:43.567493Z","iopub.status.idle":"2024-12-11T18:25:43.572914Z","shell.execute_reply.started":"2024-12-11T18:25:43.567461Z","shell.execute_reply":"2024-12-11T18:25:43.572050Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'labels'],\n    num_rows: 18962\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"id2label = {id: label for id, label in enumerate(train_ds.features[\"labels\"].names)}\nlabel2id = {label: id for id, label in id2label.items()}\nid2label,label2id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:26:22.318461Z","iopub.execute_input":"2024-12-11T18:26:22.318800Z","iopub.status.idle":"2024-12-11T18:26:22.325200Z","shell.execute_reply.started":"2024-12-11T18:26:22.318771Z","shell.execute_reply":"2024-12-11T18:26:22.324268Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"({0: 'cat', 1: 'dog'}, {'cat': 0, 'dog': 1})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import ViTImageProcessor\n\nmodel_name = \"google/vit-large-patch16-224\"\nprocessor = ViTImageProcessor.from_pretrained(model_name)\nprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:27:02.217386Z","iopub.execute_input":"2024-12-11T18:27:02.217688Z","iopub.status.idle":"2024-12-11T18:27:16.782653Z","shell.execute_reply.started":"2024-12-11T18:27:02.217664Z","shell.execute_reply":"2024-12-11T18:27:16.781789Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2639c52acb4be1876779395c428e68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be8be9db62248e2b2c790d8e19196e7"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"ViTImageProcessor {\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    ToTensor,\n    Resize,\n)\n\n# Get configurations from ViT processor\nimage_mean, image_std = processor.image_mean, processor.image_std\nsize = processor.size[\"height\"]\n\n# Normalizes the image pixels by subtracting the mean and dividing by the std from the pretrained model configurations\nnormalize = Normalize(mean=image_mean, std=image_std)\n\n# Compose: Combines a series of image transformations into one pipeline.\ntrain_transforms = Compose(\n    [\n        RandomResizedCrop(size),\n        RandomHorizontalFlip(),\n        ToTensor(),\n        normalize,\n    ]\n)\nval_transforms = Compose(\n    [\n        Resize(size),\n        CenterCrop(size),\n        ToTensor(),\n        normalize,\n    ]\n)\ntest_transforms = Compose(\n    [\n        Resize(size),\n        CenterCrop(size),\n        ToTensor(),\n        normalize,\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:27:49.132947Z","iopub.execute_input":"2024-12-11T18:27:49.133555Z","iopub.status.idle":"2024-12-11T18:27:49.139964Z","shell.execute_reply.started":"2024-12-11T18:27:49.133516Z","shell.execute_reply":"2024-12-11T18:27:49.139127Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def apply_train_transforms(examples):\n    examples[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return examples\n\n\ndef apply_val_transforms(examples):\n    examples[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return examples\n\n\ndef apply_test_transforms(examples):\n    examples[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return examples\n\ntrain_ds.set_transform(apply_train_transforms)\nval_ds.set_transform(apply_val_transforms)\ntest_ds.set_transform(apply_test_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:28:07.547175Z","iopub.execute_input":"2024-12-11T18:28:07.547475Z","iopub.status.idle":"2024-12-11T18:28:07.561565Z","shell.execute_reply.started":"2024-12-11T18:28:07.547451Z","shell.execute_reply":"2024-12-11T18:28:07.560726Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import ViTForImageClassification\n\nlabels  = dataset['train'].features['labels'].names\n\nmodel = ViTForImageClassification.from_pretrained(\n    model_name, \n    num_labels = len(labels),\n    id2label=id2label, \n    label2id=label2id, \n    ignore_mismatched_sizes=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:28:47.391160Z","iopub.execute_input":"2024-12-11T18:28:47.391469Z","iopub.status.idle":"2024-12-11T18:28:57.128259Z","shell.execute_reply.started":"2024-12-11T18:28:47.391442Z","shell.execute_reply":"2024-12-11T18:28:57.127526Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b60178b4469b4a1497030eb49a97eb09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e16dea456e4a4ab61348e422353482"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport numpy as np\n\ntrain_args = TrainingArguments(\n    output_dir=\"./output_models\",\n  per_device_train_batch_size=16,\n  evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n  num_train_epochs=3,\n  fp16=True,\n  logging_steps=10,\n  learning_rate=2e-4,\n  save_total_limit=2,\n  remove_unused_columns=False,\n  push_to_hub=False,\n  load_best_model_at_end=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:31:31.206267Z","iopub.execute_input":"2024-12-11T18:31:31.206579Z","iopub.status.idle":"2024-12-11T18:31:31.272396Z","shell.execute_reply.started":"2024-12-11T18:31:31.206551Z","shell.execute_reply":"2024-12-11T18:31:31.271727Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\n\ndef collate_fn(examples):\n    # Stacks the pixel values of all examples into a single tensor and collects labels into a tensor\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    labels = torch.tensor([example[\"labels\"] for example in examples])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# Create a DataLoader for the training dataset, with custom collation and a batch size of 4\ntrain_dl = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:36:35.962157Z","iopub.execute_input":"2024-12-11T18:36:35.962527Z","iopub.status.idle":"2024-12-11T18:36:35.968753Z","shell.execute_reply.started":"2024-12-11T18:36:35.962499Z","shell.execute_reply":"2024-12-11T18:36:35.967926Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    train_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collate_fn,\n    tokenizer=processor,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:36:36.390363Z","iopub.execute_input":"2024-12-11T18:36:36.390660Z","iopub.status.idle":"2024-12-11T18:36:36.404034Z","shell.execute_reply.started":"2024-12-11T18:36:36.390634Z","shell.execute_reply":"2024-12-11T18:36:36.402970Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1083788386.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:36:36.892910Z","iopub.execute_input":"2024-12-11T18:36:36.893241Z","iopub.status.idle":"2024-12-11T19:08:30.518487Z","shell.execute_reply.started":"2024-12-11T18:36:36.893212Z","shell.execute_reply":"2024-12-11T19:08:30.517360Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='739' max='1779' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 739/1779 31:46 < 44:50, 0.39 it/s, Epoch 1.24/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.190500</td>\n      <td>0.112126</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\"/kaggle/working/output_models/checkpoint-593\")\nfrom transformers import pipeline, ViTForImageClassification, AutoImageProcessor\n\n# Load the model\nmodel = ViTForImageClassification.from_pretrained(\"/kaggle/working/output_models/checkpoint-593\")\n\n# Load the image processor\nimage_processor = AutoImageProcessor.from_pretrained(\"/kaggle/working/output_models/checkpoint-593\")\npipe = pipeline('image-classification', model=model, image_processor=image_processor)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T19:14:54.627517Z","iopub.execute_input":"2024-12-11T19:14:54.627837Z","iopub.status.idle":"2024-12-11T19:14:54.906980Z","shell.execute_reply.started":"2024-12-11T19:14:54.627812Z","shell.execute_reply":"2024-12-11T19:14:54.906090Z"}},"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"res=pipe('/kaggle/input/bbbbnnnn/OIP.jpg')\nres","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T19:19:48.352521Z","iopub.execute_input":"2024-12-11T19:19:48.352914Z","iopub.status.idle":"2024-12-11T19:19:49.853541Z","shell.execute_reply.started":"2024-12-11T19:19:48.352882Z","shell.execute_reply":"2024-12-11T19:19:49.852766Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[{'label': 'cat', 'score': 0.9905894994735718},\n {'label': 'dog', 'score': 0.011865032836794853}]"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"dic={}\ndic[res[0]['label']]=res[0]['score']\ndic[res[1]['label']]=res[1]['score']\ndic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T19:21:17.355463Z","iopub.execute_input":"2024-12-11T19:21:17.356531Z","iopub.status.idle":"2024-12-11T19:21:17.362981Z","shell.execute_reply.started":"2024-12-11T19:21:17.356495Z","shell.execute_reply":"2024-12-11T19:21:17.362152Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"{'cat': 0.9905894994735718, 'dog': 0.011865032836794853}"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}